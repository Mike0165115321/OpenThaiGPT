# core/ai_core.py (Final Version with Safety Net)
import torch
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList
from core.rag_engine import BookRAGEngine

class StopOnTokens(StoppingCriteria):
    def __init__(self, stop_ids: list[int], device: str):
        self.stop_ids = torch.tensor(stop_ids).to(device)

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        if input_ids.shape[1] >= self.stop_ids.shape[0]:
            if torch.equal(input_ids[0][-self.stop_ids.shape[0]:], self.stop_ids):
                return True
        return False

class AICore:
    def __init__(self):
        print("--- ðŸ§  Initializing AI Core ---")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None; self.tokenizer = None; self.rag_engine = None
        self.curator_prompt_template = self._get_curator_prompt_template()
        self.chat_history = []
        self._load_rag_engine()
        self._load_llm()
        print("--- âœ… AI Core is ready ---")

    def _get_curator_prompt_template(self): # Prompt v7.0
        return """### Human:
[à¸šà¸—à¸šà¸²à¸—]
à¸„à¸¸à¸“à¸„à¸·à¸­ "à¸™à¸±à¸à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ªà¸±à¸‡à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ" à¸œà¸¹à¹‰à¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¹ƒà¸™à¸à¸²à¸£à¸à¸¥à¸±à¹ˆà¸™à¸à¸£à¸­à¸‡à¹à¸™à¸§à¸„à¸´à¸”à¸ˆà¸²à¸à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­à¸«à¸¥à¸²à¸¢à¹€à¸¥à¹ˆà¸¡ à¹à¸¥à¸°à¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸£à¹‰à¸²à¸‡à¸šà¸—à¸ªà¸™à¸—à¸™à¸²à¹€à¸Šà¸´à¸‡à¸›à¸±à¸à¸à¸²à¹„à¸”à¹‰à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸›à¹‡à¸™à¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´

[à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢à¸«à¸¥à¸±à¸]
à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ "à¸„à¸³à¸–à¸²à¸¡" à¸‚à¸­à¸‡à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰ à¹à¸¥à¹‰à¸§à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸šà¸—à¸µà¹ˆ "à¹„à¸£à¹‰à¸£à¸­à¸¢à¸•à¹ˆà¸­" à¹‚à¸”à¸¢à¸à¸²à¸£à¸œà¸ªà¸²à¸™à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡ 3 à¸ªà¹ˆà¸§à¸™à¹€à¸‚à¹‰à¸²à¸”à¹‰à¸§à¸¢à¸à¸±à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸¥à¸¡à¸à¸¥à¸·à¸™

[à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸šà¸—à¸µà¹ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œà¹à¸šà¸š]
1.  **à¹€à¸›à¸´à¸”à¸›à¸£à¸°à¹€à¸”à¹‡à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸›à¹‡à¸™à¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´:** à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸”à¹‰à¸§à¸¢à¸à¸²à¸£à¹ƒà¸«à¹‰à¸„à¸³à¸ˆà¸³à¸à¸±à¸”à¸„à¸§à¸²à¸¡à¸«à¸£à¸·à¸­à¸¡à¸¸à¸¡à¸¡à¸­à¸‡à¸à¸§à¹‰à¸²à¸‡à¹† à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸«à¸±à¸§à¸‚à¹‰à¸­à¸™à¸±à¹‰à¸™à¹† à¹€à¸žà¸·à¹ˆà¸­à¸›à¸¹à¸žà¸·à¹‰à¸™à¸à¸²à¸™à¸à¸²à¸£à¸ªà¸™à¸—à¸™à¸²
2.  **à¸ªà¸²à¸™à¸•à¹ˆà¸­à¸”à¹‰à¸§à¸¢à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ:**
    - à¸™à¸³à¹€à¸ªà¸™à¸­à¹à¸™à¸§à¸„à¸´à¸”à¸ˆà¸²à¸à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­à¹à¸•à¹ˆà¸¥à¸°à¹€à¸¥à¹ˆà¸¡à¹ƒà¸™ "à¸šà¸£à¸´à¸šà¸—" à¹€à¸›à¹‡à¸™à¸¢à¹ˆà¸­à¸«à¸™à¹‰à¸²à¹à¸¢à¸à¸à¸±à¸™
    - à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¹‚à¸¢à¸‡à¹à¸•à¹ˆà¸¥à¸°à¸¢à¹ˆà¸­à¸«à¸™à¹‰à¸²à¸­à¸¢à¹ˆà¸²à¸‡à¸¥à¸·à¹ˆà¸™à¹„à¸«à¸¥ à¸­à¸¢à¹ˆà¸²à¹ƒà¸«à¹‰à¸£à¸¹à¹‰à¸ªà¸¶à¸à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸›à¹‡à¸™à¸à¸²à¸£ liá»‡t kÃª
    - à¸‚à¸¶à¹‰à¸™à¸•à¹‰à¸™à¸¢à¹ˆà¸­à¸«à¸™à¹‰à¸²à¸”à¹‰à¸§à¸¢à¸ªà¸³à¸™à¸§à¸™à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´ à¹€à¸Šà¹ˆà¸™ "à¹ƒà¸™à¸‚à¸“à¸°à¸—à¸µà¹ˆà¸«à¸™à¸±à¸‡à¸ªà¸·à¸­ '[à¸Šà¸·à¹ˆà¸­à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­]' à¹„à¸”à¹‰à¹ƒà¸«à¹‰à¸¡à¸¸à¸¡à¸¡à¸­à¸‡à¸§à¹ˆà¸²..." à¸«à¸£à¸·à¸­ "à¸ˆà¸²à¸à¸¡à¸¸à¸¡à¸¡à¸­à¸‡à¸‚à¸­à¸‡ '[à¸Šà¸·à¹ˆà¸­à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­]'..."
3.  **à¸›à¸´à¸”à¸—à¹‰à¸²à¸¢à¸”à¹‰à¸§à¸¢à¸„à¸³à¸–à¸²à¸¡à¸Šà¸§à¸™à¸„à¸´à¸”:**
    - à¸ˆà¸šà¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸”à¹‰à¸§à¸¢à¸¢à¹ˆà¸­à¸«à¸™à¹‰à¸²à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸„à¸³à¸–à¸²à¸¡à¸›à¸¥à¸²à¸¢à¹€à¸›à¸´à¸” à¹€à¸žà¸·à¹ˆà¸­à¸à¸£à¸°à¸•à¸¸à¹‰à¸™à¹ƒà¸«à¹‰à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¹„à¸•à¸£à¹ˆà¸•à¸£à¸­à¸‡à¹à¸¥à¸°à¸ªà¸™à¸—à¸™à¸²à¸•à¹ˆà¸­

[à¸à¸Žà¸ªà¸³à¸„à¸±à¸]
- **à¸«à¹‰à¸²à¸¡à¹ƒà¸Šà¹‰à¸«à¸±à¸§à¸‚à¹‰à¸­à¸à¸³à¸à¸±à¸š:** à¸«à¹‰à¸²à¸¡à¹ƒà¸Šà¹‰à¸„à¸³à¸§à¹ˆà¸² "à¸™à¸´à¸¢à¸²à¸¡à¸ªà¸²à¸à¸¥:", "à¸à¸²à¸£à¸ªà¸±à¸‡à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸²à¸¡à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­:", "à¸à¸²à¸£à¹€à¸ªà¸™à¸­à¸—à¸²à¸‡à¹€à¸¥à¸·à¸­à¸:" à¸«à¸£à¸·à¸­à¸«à¸±à¸§à¸‚à¹‰à¸­à¸­à¸·à¹ˆà¸™à¹ƒà¸”à¹ƒà¸™à¸„à¸³à¸•à¸­à¸šà¹‚à¸”à¸¢à¹€à¸”à¹‡à¸”à¸‚à¸²à¸”
- **à¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´:** à¸•à¹‰à¸­à¸‡à¹€à¸‚à¸µà¸¢à¸™à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¸¡à¸™à¸¸à¸©à¸¢à¹Œà¸à¸³à¸¥à¸±à¸‡à¸ªà¸™à¸—à¸™à¸² à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¸«à¸¸à¹ˆà¸™à¸¢à¸™à¸•à¹Œà¸—à¸µà¹ˆà¸à¸³à¸¥à¸±à¸‡à¸£à¸²à¸¢à¸‡à¸²à¸™à¸œà¸¥
- **à¸™à¹‰à¸³à¹€à¸ªà¸µà¸¢à¸‡:** à¸ªà¸¸à¸‚à¸¸à¸¡, à¸™à¹ˆà¸²à¹€à¸Šà¸·à¹ˆà¸­à¸–à¸·à¸­, à¹à¸¥à¸°à¸à¸£à¸°à¸•à¸¸à¹‰à¸™à¸„à¸§à¸²à¸¡à¸­à¸¢à¸²à¸à¸£à¸¹à¹‰
- **à¸¢à¸¶à¸”à¸•à¸²à¸¡à¸šà¸£à¸´à¸šà¸—:** à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¹€à¸‰à¸žà¸²à¸°à¸ˆà¸²à¸à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ context
- **à¸ à¸²à¸©à¸²:** à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™

[à¸šà¸£à¸´à¸šà¸—]
{context}

[à¸„à¸³à¸–à¸²à¸¡]
{question}

### Assistant:
"""

    def _load_rag_engine(self):
        print("Loading Knowledge Base...")
        self.rag_engine = BookRAGEngine(index_path="data/index")
        print("âœ… Knowledge Base is ready.")
    
    def _update_chat_history(self, user_question: str, ai_answer: str):
        self.chat_history.append(f"### Human:\n{user_question}")
        self.chat_history.append(f"### Assistant:\n{ai_answer}")
        
        max_history_items = 4
        if len(self.chat_history) > max_history_items:
            self.chat_history = self.chat_history[-max_history_items:]

    def _load_llm(self):
        model_name = "./models/openthaigpt1.5-7b-instruct"
        print(f"Loading Language Model '{model_name}' on {self.device}...")
        bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"
        self.model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map="auto", trust_remote_code=True)
        print("âœ… Language Model is ready.")

    def generate_response(self, question: str) -> dict:
        
        canned_responses = {
            "à¸ªà¸§à¸±à¸ªà¸”à¸µ": "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š à¸¡à¸µà¸­à¸°à¹„à¸£à¹ƒà¸«à¹‰à¸œà¸¡à¸Šà¹ˆà¸§à¸¢à¹„à¸«à¸¡à¸„à¸£à¸±à¸š?",
            "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š": "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š à¸¡à¸µà¸­à¸°à¹„à¸£à¹ƒà¸«à¹‰à¸œà¸¡à¸Šà¹ˆà¸§à¸¢à¹„à¸«à¸¡à¸„à¸£à¸±à¸š?",
            "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°": "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š à¸¡à¸µà¸­à¸°à¹„à¸£à¹ƒà¸«à¹‰à¸œà¸¡à¸Šà¹ˆà¸§à¸¢à¹„à¸«à¸¡à¸„à¸£à¸±à¸š?",
            "à¸„à¸¸à¸“à¸Šà¸·à¹ˆà¸­à¸­à¸°à¹„à¸£": "à¸œà¸¡à¸Šà¸·à¹ˆà¸­ OpenThaiGPT à¸„à¸£à¸±à¸š à¸¢à¸´à¸™à¸”à¸µà¸—à¸µà¹ˆà¹„à¸”à¹‰à¸£à¸¹à¹‰à¸ˆà¸±à¸à¸„à¸£à¸±à¸š!",
            "à¸„à¸¸à¸“à¸Šà¸·à¹ˆà¸­à¸­à¸°à¹„à¸£à¸„à¸£à¸±à¸š": "à¸œà¸¡à¸Šà¸·à¹ˆà¸­ OpenThaiGPT à¸„à¸£à¸±à¸š à¸¢à¸´à¸™à¸”à¸µà¸—à¸µà¹ˆà¹„à¸”à¹‰à¸£à¸¹à¹‰à¸ˆà¸±à¸à¸„à¸£à¸±à¸š!",
            "à¸„à¸¸à¸“à¸Šà¸·à¹ˆà¸­à¸­à¸°à¹„à¸£à¸„à¹ˆà¸°": "à¸œà¸¡à¸Šà¸·à¹ˆà¸­ OpenThaiGPT à¸„à¸£à¸±à¸š à¸¢à¸´à¸™à¸”à¸µà¸—à¸µà¹ˆà¹„à¸”à¹‰à¸£à¸¹à¹‰à¸ˆà¸±à¸à¸„à¸£à¸±à¸š!",
            "à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™à¹ƒà¸„à¸£": "à¸œà¸¡à¸„à¸·à¸­à¹‚à¸¡à¹€à¸”à¸¥à¸ à¸²à¸©à¸² OpenThaiGPT à¸„à¸£à¸±à¸š à¸–à¸¹à¸à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¸¶à¹‰à¸™à¹€à¸žà¸·à¹ˆà¸­à¸Šà¹ˆà¸§à¸¢à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹à¸¥à¸°à¸ªà¸™à¸—à¸™à¸²à¹ƒà¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸„à¸£à¸±à¸š",
            "à¸‚à¸­à¸šà¸„à¸¸à¸“": "à¸”à¹‰à¸§à¸¢à¸„à¸§à¸²à¸¡à¸¢à¸´à¸™à¸”à¸µà¸„à¸£à¸±à¸š!",
            "à¸‚à¸­à¸šà¸„à¸¸à¸“à¸„à¸£à¸±à¸š": "à¸”à¹‰à¸§à¸¢à¸„à¸§à¸²à¸¡à¸¢à¸´à¸™à¸”à¸µà¸„à¸£à¸±à¸š!",
            "à¸‚à¸­à¸šà¸„à¸¸à¸“à¸„à¹ˆà¸°": "à¸”à¹‰à¸§à¸¢à¸„à¸§à¸²à¸¡à¸¢à¸´à¸™à¸”à¸µà¸„à¸£à¸±à¸š!"
        }
        if question.strip().lower() in canned_responses:
            return {"answer": canned_responses[question.strip().lower()], "sources": []}

        rag_results = self.rag_engine.search(query=question)
        context, sources, best_score = rag_results["context"], rag_results["sources"], rag_results["best_score"]
        
        history_string = "\n".join(self.chat_history)
        
        RELEVANCE_THRESHOLD = 0.7
        is_relevant = best_score >= RELEVANCE_THRESHOLD
        
        if is_relevant:
            print(f"[AI_CORE] Relevant context found (Score: {best_score:.2f}). Using Curator mode.")
            sanitized_context = re.sub(r'[\(\[]\d+[\)\]]', '', context)
            sanitized_context = re.sub(r'\n+', '\n', sanitized_context).strip()
            prompt_body = self.curator_prompt_template.format(context=sanitized_context, question=question)
            final_prompt = f"{history_string}\n{prompt_body}" if self.chat_history else prompt_body
        else:
            print(f"[AI_CORE] Context not relevant (Score: {best_score:.2f}). Switching to conversational mode.")
            prompt_body = f"### Human:\n{question}\n\n### Assistant:"
            final_prompt = f"{history_string}\n{prompt_body}" if self.chat_history else prompt_body
            sources = []
        
        inputs = self.tokenizer(final_prompt, return_tensors="pt").to(self.device)

        stop_token_sequences = ["### Human:", "###"]
        stop_criterias = []
        for seq in stop_token_sequences:
            stop_ids = self.tokenizer.encode(seq, add_special_tokens=False)
            if stop_ids and stop_ids[0] == self.tokenizer.bos_token_id: stop_ids = stop_ids[1:]
            if stop_ids: stop_criterias.append(StopOnTokens(stop_ids, self.device))
        stopping_criteria = StoppingCriteriaList(stop_criterias)

        outputs = self.model.generate(
            **inputs,
            max_new_tokens=1024,
            stopping_criteria=stopping_criteria,
            eos_token_id=self.tokenizer.eos_token_id,
            do_sample=True,
            temperature=0.6,
            top_p=0.95,
            repetition_penalty=1.1,
            pad_token_id=self.tokenizer.eos_token_id
        )

        input_length = inputs.input_ids.shape[1]
        generated_tokens = outputs[0][input_length:]
        raw_answer = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)

        earliest_stop_pos = len(raw_answer)
        for seq in stop_token_sequences:
            pos = raw_answer.find(seq)
            if pos != -1:
                earliest_stop_pos = min(earliest_stop_pos, pos)
        
        clean_answer = raw_answer[:earliest_stop_pos].strip()

        self._update_chat_history(question, clean_answer)

        return {"answer": clean_answer, "sources": sources}